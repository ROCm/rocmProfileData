## First steps with rpd

After tracing the workload with rpd, a trace database (typically `trace.rpd`) will have been created. This is a simple `sqlite3` compatible database which can be opened with the sqlite command.

```
sqlite3 trace.rpd
```

First, we enable headers;
```
sqlite> .headers on
```

### Kernel-level hot spot location
The top-10 kernels by GPU time can be accessed from the `sqlite>` prompt with
```
sqlite> select * from top limit 10;
```

Output may look like this example:
```
Name|TotalCalls|TotalDuration|Ave|Percentage
mscclKernel_Sum_half_Simple_false(ncclDevComm*, mscclAlgo*, mscclWork*)|52224|125822179|2409|18.8200893749884
ncclDevKernel_Generic(ncclDevComm*, channelMasks, ncclWork*)|26592|69872574|2627|10.4513218907667
void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::sum_functor<c10::Half, float, c10::Half>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::Half, 4>)|204544|41584720|203|6.22011279934209
void at::native::tensor_kernel_scan_outer_dim<long, std::plus<long> >(long*, long const*, unsigned int, unsigned int, unsigned int, long, std::plus<long>)|13056|41224082|3157|6.16616971000224
Cijk_Ailk_Bljk_HHS_BH_Bias_AS_SAV_UserArgs_MT128x256x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_CADS0_EPS0_GRVWA8_GRVWB8_GSUAMB_IU1_K1_LBSPPA2048_LBSPPB128_LBSPPM0_LPA0_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT4_8_MO40_NTn1_NTA0_NTB0_NTC0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW4_TLDS1_USFGROn1_VWA4_VWB1_WSGRA0_WSGRB0_WG32_8_1_WGMXCC1|47416|31879774|672|4.76847732971161
Cijk_Alik_Bljk_HHS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_CADS0_EPS0_GRVWA4_GRVWB4_GSUAMB_IU1_K1_LBSPPA1024_LBSPPB128_LBSPPM0_LPA4_LPB4_LPM0_LRVW4_LWPMn1_MIAV0_MIWT8_7_MO40_NTn1_NTA0_NTB0_NTC0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW8_TLDS1_USFGROn1_VWA8_VWB1_WSGRA0_WSGRB0_WG32_8_1_WGMXCC1|17064|25540909|1496|3.82032955696057
Cijk_Ailk_Bjlk_HHS_BH_Bias_AS_SAV_UserArgs_MT64x64x128_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_CADS0_EPS0_GRVWA8_GRVWB8_GSUAMB_IU1_K1_LBSPPA512_LBSPPB512_LBSPPM0_LPA32_LPB32_LPM0_LRVW4_LWPMn1_MIAV0_MIWT2_2_MO40_NTn1_NTA0_NTB0_NTC0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS0_USFGROn1_VWA2_VWB2_WSGRA0_WSGRB0_WG32_8_1_WGMXCC1|156672|25363119|161|3.79373637262398
Cijk_Ailk_Bjlk_HHS_BH_Bias_AS_SAV_UserArgs_MT128x256x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_CADS0_EPS0_GRVWA8_GRVWB8_GSUAMB_IU1_K1_LBSPPA1024_LBSPPB2048_LBSPPM0_LPA0_LPB0_LPM0_LRVW4_LWPMn1_MIAV0_MIWT4_8_MO40_NTn1_NTA0_NTB0_NTC0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW4_TLDS0_USFGROn1_VWA4_VWB8_WSGRA0_WSGRB0_WG32_8_1_WGMXCC1|26124|25165450|963|3.76416970247375
void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#10}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})|430204|22419619|52|3.3534567335353
Cijk_Alik_Bljk_HHS_BH_Bias_AS_SAV_UserArgs_MT256x128x32_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_CADS0_EPS0_GRVWA4_GRVWB4_GSUAMB_IU1_K1_LBSPPA512_LBSPPB128_LBSPPM0_LPA4_LPB4_LPM0_LRVW4_LWPMn1_MIAV0_MIWT8_4_MO40_NTn1_NTA0_NTB0_NTC0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW8_TLDS1_USFGROn1_VWA8_VWB1_WSGRA0_WSGRB0_WG32_8_1_WGMXCC1|124107|21138895|170|3.16189010316332
```

The initial focus is on the `Percentage` column. Here, we see a `mscclKernel_Sum_half_Simple_false` take 18.8% of total GPU time, followed by a `ncclDevKernel_Generic` kernel taking 10.5%. Based on the `TotalCalls`, `TotalDuration`, and `Ave` columns, these are relatively long kernels at averages of 2409 us and 2627 us, respectively. In comparison, the `at::native::reduce_kernel` is very short lived at an average of 203 us but is invoked often enough to still account for `6.2%` of GPU time.

Overall, this view highlights the kernel-level hot spots of the application and whether these are, on average, caused by numerous short or less frequent long kernel executions.

### Grouping kernels and assessing their total impact on performance
Next, we can group kernels into sets and assess their total time on GPU with simple string matching.

For the total contribution of matrix multiplication kernels:
```
sqlite> select sum(Percentage) from top where Name like 'Cijk%';
```
Which in our example will return:
```
sum(Percentage)
33.6759566310081`
```
I.e., 33.6% of GPU time is spent in matrix multiplications.

Similarly, for all elementwise kernels:
```
sqlite> select sum(Percentage) from top where Name like '%elementwise%';
```
will return 16.0% spent in elementwise.


For all collective communication kernels:
```
sqlite> select sum(Percentage) from top where Name like '%ccl%';
```
at 29.3% and the contributing kernels:
```
sqlite> select * from top where Name like '%ccl%';
```
with output
```
Name|TotalCalls|TotalDuration|Ave|Percentage
mscclKernel_Sum_half_Simple_false(ncclDevComm*, mscclAlgo*, mscclWork*)|52224|125822179|2409|18.8200893749884
ncclDevKernel_Generic(ncclDevComm*, channelMasks, ncclWork*)|26592|69872574|2627|10.4513218907667
mscclKernel_Sum_uint8_t_LL_false(ncclDevComm*, mscclAlgo*, mscclWork*)|8|3559|444|0.000532429084356098
```

For all reduction kernels:
```
sqlite> select sum(Percentage) from top where Name like '%reduce_kernel%';
```
at 8.8%.

Other groups may apply to and be constructed for a specific workload.

Overall, grouping kernels and assessing their overall impact helps in understanding what drives the performance (and potential gains) in a workload. It also allows trivial model of end-to-end gains. I.e., if matrix multiplications would get 2x faster but are only 33.6% of time on GPU, the end-to-end uplift would be under 20%. 

### Busy time or GPU execution gaps
A high-level view of GPU busy time (and its inverse, gaps in GPU execution) can be achieved from looking at the `busy` table.
```
sqlite> select * from busy;
gpuId|GpuTime|WallTime|Busy
2|81111298846|126018228685|0.643647349216032
3|74045687063|126018228685|0.587579176724404
4|84577265111|126018228685|0.671151038969232
5|85360254842|126018228685|0.677364344291569
6|85621957104|126018228685|0.67944104593014
7|85638792053|126018228685|0.679574637309544
8|85452706841|126018228685|0.678097984178153
9|86744546744|126018228685|0.688349198756237
```
with per-device resolution. However, these numbers are typically pessimistic unless the profiling data collection was started and stopped exactly around the relevant exection. I.e., here the busy time per device is around 60-70% which would be reason for concern normally. However, here it is caused by setup time on host (e.g., data set loading) considered as "gap" time because specific starting and stopping of the data collection did not take place.

### Summary
All commands here have been added to the `rpd-101.sql` sqlite script which can be directly invoked on the profiling database from the Linux/macOS command line:

```
sqlite3 trace.db < rpd-101.sql
```
